---
title: "Quick intro to statistics"
author: "Taavi Päll, Ülo Maiväli"
date: "2018-JUN-05 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = FALSE, message = FALSE, fig.align = 'center', dev = 'svg')
library(tidyverse)
ggplot2::theme_set(theme_gray(base_size = 18))
```

class: inverse, center, middle

# What is statistics

---

## Questions that statistics asks

- How your data looks like and what are the parameter values describing your distribution? *For example, mean, median, variation, covariation*

- What should we believe, based on our sample, about the real parameter value in the whole population? *For example, when in our sample mean height is 178 cm, then what is the probability that real mean height of the population is > 185 cm?*

- Does the structure of our statistical model support our scientific hypothesis? *For example, when covariance of weight and height can be described by linear regression model, then we can prefer scientific hypothesis that provides mechanism for such relationship*

- What and how well our model predicts the future? *For example, can our model reasonably well predict outcome using new data?*

???

- For example, covariation of measured weight and height can be measured with help of correlation coeficient.

---

## Statistics answers scientific questions indirectly

- Main function of statistics is to quantitate the uncertainty when we seek answers to above questions (p-values, posterior distribution of effect size)

- Statistics never answers scientific questions or questions about the state of real world directly, it's more like what is the probability of data given that null hypothesis is true

- Answers that statistics provides remain always confined to our data and models, putting emphasis on representative sampling. *Therefore one should always prefer well collected, rich data and flexible models in order to keep divergency between model coeficients and real world as small as possible (better data means better models)*

- Statistical inferences are always uncertain

---

## Branches of statistics

Statistics can be divided into three broad categories: descriptive, exploratory, and inferential:

1. Descriptive statistics includes summary statistics that are used to summarise a set of observations, in order to communicate the largest amount of information as simply as possible 

1. Exploratory stats creates new hypotheses based on your data by using iterative visualisation and modeling 

1. Inferential stats consists formal framework to test proposed hypotheses

---
class: inverse, center, middle

# Descriptive stats

---

## Summary stats characterise your sample

- Typical observation of the sample (mean, median, mode)

- Variance of observations (standard deviation, MAD),

- Covariance of variables (correlation coeficient)

```{r, fig.height=3}
set.seed(506)
mode <-  function(x, adjust = 1) {
  x <- na.omit(x)
  dx <- density(x, adjust = adjust)
  dx$x[which.max(dx$y)]
}
d <- data_frame(values = rlnorm(1000, sdlog = 0.6))
ggplot(data = d) +
  geom_density(mapping = aes(x = values, y = ..density..)) +
  geom_vline(xintercept = median(d$values), linetype = "dashed", size = 1) +
  geom_vline(xintercept = mean(d$values), size = 1) +
  geom_vline(xintercept = mode(d$values), linetype = "dotted", size = 1) +
  labs(caption = "MEAN (solid line),\nMEDIAN (dashed line),\nMODE (dotted line) of an log-normal distribution.")
```

Which of these summary statistics represents best your intuition about central tendency of this distribution?

???

- Kui valim on normaaljaotusega (histogramm on sümmeetriline), hinda tüüpilist liiget läbi aritmeetilise keskmise (mean).

Muidu kasuta mediaani (median). Kui valim on liiga väike, et jaotust hinnata (aga > 4), eelista mediaani. Mediaani saamiseks järjestatakse mõõdetud väärtused suuruse järgi ja võetakse selle rea keskmine liige. Mediaan on vähem tundlik ekstreemsete väärtuste (outlierite) suhtes kui mean.

Valimi kõige levinumat esindajat iseloomustab mood ehk jaotuse tipp. Seda on aga raskem täpselt määrata ja mitmetipulisel jaotusel on mitu moodi. Töötamisel posterioorsete jaotustega on mood sageli parim lahendus.

---

## Skim your dataset

Let's have a look at the summary stats of some variables of Starwars movie characters from `tidyverse::starwars` data:
```{r}
knitr::include_graphics("img/skim.png")
```
 
---

## Skim your dataset

Let's have a look at the summary stats of some variables of Starwars movie characters from `tidyverse::starwars` data:
```{r}
knitr::include_graphics("img/skim-rect.png")
```
 
---

## Variance of random variables

- Arithmetic mean is accompanied with standard deviation (SD). *SD has same dimension as your data and mean*

- Preferred format for displaying mean and SD in text is: "**mean (SD)**" (not "mean ( $\pm$ SD)") 

- 1 SD covers 68% of normal distribution, 2 SD - 96% and 3 SD - 99%. *Normal distribution has "thin" tails and for example only 1 out of 1 million data points is 5 SD away from mean*

---

## Example: human IQ

- Human IQ is normally distributed with mean = 100 and sd = 15

- When your IQ is 115 (mean IQ of students admitting university is 1 SD away from population mean), then the probability that random person you encounter on the street has higher IQ than you is 16% ((100% - 68%) / 2 = 16%)


```{r, fig.height = 4}
iq <- data_frame(IQ = rnorm(1000, 100, 15))
iqd <- density(iq$IQ)
smarter_than_you <- data_frame(x = iqd$x, y = iqd$y) %>% 
  filter(x >= 115)
ggplot(data = iq) +
  geom_density(mapping = aes(x = IQ)) +
  geom_vline(xintercept = 115, linetype = "dashed", size = 1) +
  geom_ribbon(data = smarter_than_you, mapping = aes(x = x, ymax = y), ymin = 0, alpha = 0.5) +
  labs(title = "Normal distribution\nwith mean=100 and sd=15")
```


---

## When your distribution is not that "normal"

- When your real life distribution has long or "fat" tails or has outliers then formula for calculating SD for normal distribution $s = \sqrt{\frac{\sum_{i=1}^N (x_i- \bar x)^2} {N - 1}}$ will overestimate SD and sample variance  

- When your data can only have positive values, then SD>mean/2 suggests that your data is not compatible with the normal model 

- Simplest way to fix this situation when there are only positive values in your dataset is to **log transform** your data

```{r, fig.height=3}
d <- data_frame(`log-normal` = rlnorm(1000)) %>% 
  mutate(`log-transformed` = log(`log-normal`)) %>% 
  gather()
ggplot(data = d) +
  geom_histogram(mapping = aes(x = value), bins = 30) +
  facet_wrap(~key, scales = "free_x")
```



???

- normal model predicts negative values with relatively high frequency
- igale jaotusele, mida me oskame integreerida, saab ka integraali abil õige katvusega standardhälbe arvutada. 
- Seega tasub meeles pidada, et tavapärane viis standardhälbe arvutamiseks sd() abil kehtib normaaljaotuse mudeli piirides ja ei kusagil mujal! 
- Siiski, kui arvutada standardhälbe sd()-ga, võib olla kindel, jaotusest sõltumata hõlvavad 2 SD-d vähemalt 75% andmejaotusest. 
- Kui andmed ei sobi normaaljaotusesse ja te ei ole rahul tulemusega, mille tõlgendus on nii ebakindel kui 75 protsenti kuni 96+ protsenti, võib pakkuda kahte alternatiivset lahendust:


---

## SD calculated using original log-normal data is way off

```{r, fig.height=5}
original <- filter(d, key == "log-normal")
lower <- mean(original$value) - sd(original$value)
upper <- mean(original$value) + sd(original$value)
frac_data <- sum(original$value > lower & original$value < upper) / 1000
ggplot(data = original) +
  geom_density(mapping = aes(x = value)) +
  geom_vline(xintercept = c(lower,  upper), linetype = "longdash") +
  geom_label(label = frac_data, x = 1, y = 0.1) +
  labs(caption = "Dashed lines: 1*SD")
```

---

## Mean and SD from log transformed data

- Calculate mean and SD using log-transformed data and then convert them back to linear scale with anti-log. *For example, when $log_2(10) = 3.32$, then anti-log is $2^{3.32} = 10$*

- In this case, geometric mean and **geometric/multiplicative SD** is presented in the original scale:
    - multiplicative SD = geometric mean x SD; geometric mean / SD
    - however, geometric SD itself is multiplicative factor and therefore dimensionless
  - Geometric mean is always smaller than arthmetic mean 
  - SD interval is now assymetric and SD is always > 0

- This procedure ensures that ~68% of log normal data will remain in 1 SD range and ~96% data will remain into 2 SD range

.footnote[Example use of multiplicative SD in a research paper: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3876377/]

---



```{r, echo=TRUE}
# calculate mean and sd using log-trans data
dsum <- filter(d, key == "log-transformed") %>% 
  summarise_at(vars(value), funs(mean, sd))

# convert calculated mean and sd back to original scale using exp
(bounds <- transmute_at(dsum, vars(mean, sd), exp) %>% 
  transmute(lower_1sd = mean / sd, upper_1sd = mean * sd,
            lower_2sd = mean / (2 * sd), upper_2sd = mean * (2 * sd)))
```

---

```{r, fig.height=5}
ggplot(data = original) +
  geom_density(mapping = aes(x = value)) +
  geom_vline(data = gather(select(bounds, ends_with("1sd"))), mapping = aes(xintercept = value), linetype = "longdash") +
  geom_vline(data = gather(select(bounds, ends_with("2sd"))), mapping = aes(xintercept = value), linetype = "dotted") +
  labs(caption = "Dashed lines: 1*SD\ndotted lines: 2*SD")
```


---


```{r, echo=TRUE}
# test calculated sd boundaries on the original data
filter(d, key == "log-normal") %>% 
  summarise(fraction_data_1SD = sum(value > bounds$lower_1sd & value < bounds$upper_1sd) / 1000,
            fraction_data_2SD = sum(value > bounds$lower_2sd & value < bounds$upper_2sd) / 1000)
```

.footnote[Code can be found at [rstats-tln/fmtdk-course/master/stats.Rmd](https://raw.githubusercontent.com/rstats-tln/fmtdk-course/master/stats.Rmd?token=AEl8JvIQ0g8KRAUmbaAytLfPJVFc8OTdks5bGTe0wA%3D%3D) file]

---

## Testing assumption of normality

- Certain tests, such as the t-test, require a normal distribution and therefore you might feel that you need to determine empirically whether your data has normal distribution

- Before you start with various normality tests, keep in mind that when your sample is not normal it does not mean automatically that the undelying population is also non-normal

- However, you would expect your sample to have the same distribution as the population. If it isn't, either your sample is very small or something went wrong somewhere

- **Misconception: If your statistical analysis requires normality, it is a good idea to use a preliminary hypothesis test to screen for departures from normality**

.footnote[
[Do statistical tests require a normal distribution in the sample or in the population?](https://stats.stackexchange.com/q/289092/72998)
]

???

- Shapiro’s test, Anderson Darling, and others are null hypothesis tests against the the assumption of normality. 
- These should not be used to determine whether to use normal theory statistical procedures. 
- In fact they are of virtually no value to the data analyst. 
- Under what conditions are we interested in rejecting the null hypothesis that the data are normally distributed? 
- I, personally, have never come across a situation where a normal test is the right thing to do. 
- **The problem is that when the sample size is small, even big departures from normality are not detected, and when your sample size is large, even the smallest deviation from normality will lead to a rejected null.**

https://www.r-bloggers.com/normality-tests-don%E2%80%99t-do-what-you-think-they-do/


---

## Q-Q plot to compare data to theoretical distribution

Q-Q plot allows visual evaluation of your variable against theoretical quantiles of a comparison distribution, e.g. normal distribution. R has at least two function to draw a Q-Q plot `qqplot` from **stats** package and `qqPlot` from **car** package. 

Let's compare the body mass index (BMI) of Starwars characters to normal distribution

```{r, fig.height=3.5}
library(car)
sw <- filter(starwars, !is.na(height), !is.na(mass)) %>% 
  mutate(BMI = mass / (height / 100)^2)
o <- qqPlot(sw$BMI, ylab = "Starwars character BMI")
```

- Seems like `r sw$name[o[1]]` is way off the line (15)
- You can make [Q-Q plot also in MS Excel](https://youtu.be/nX6-j6lY9qc)


---

###  Normality tests are of virtually no value to the data analyst

- In case of Jabba the Hutt, even log-transformnation does not help to restore intergalactic BMI distribution close to normal. 
- However, we might expect that among Hutts in Nal Hutta (Jabbas' homeworld) BMi is still normally distributed, it has just different parameters from many other species (mean BMI is 440-something)
- We can still go on with this data and model, for example, the relationship between BMI and species or eye-color


```{r, fig.height=3.5}
o <- qqPlot(log10(sw$BMI), ylab = "Starwars character BMI, Log10")
```


---

## Summary of descriptive statistics

- Descriptive statistics is usually the first sanity check of your data

- Just by calculating simple arithmetic mean and standard deviation and looking at the distribution of your data (histogram and Q-Q plot) you can learn about your variables

- However, before you go on with exploring and modeling, it's important to understand **what types of variables** to you have in your dataset at hand and **which variables are dependent and which ones are independent**

---

class: inverse, center, middle

# Linear models

---

## Predictor and predicted variables

- Suppose we want to predict someone's weight from height. In this case, weight is the predicted variable and height is predictor variable. 

- The difference between predicted and predictor variables is that the predicted variables values are modeled as a function of values of predictor variables. 

- The value of the predictor variable comes from outside of the system being modeled, whereas the value of predicted variable depends on the value of predictor variable

- Therefore, mathematically, not in real life, predictor variable can be called **"independent"** variable and predicted variable can be called **"dependent"** variable


???

- Or, suppose we want to predict average score of an elementary school student based on PISA test result and family income, in this case average score is dependent variable and PISA test result and family income are independent variables. 

- Or, suppose we want to predict blood pressure of patients who either take drug A, or take drug B, or take a placebo, or merely wait. In this case blood pressure is the predicted variable and treatment category is the predictor


---

## Identify predicted and predictors first

- **The first thing you do in statistical inference, is to identify what variables are you interested in predicting on the basis of what predictors** 

- To rephrase, the key conceptial difference between independent and dependent variables is that the value of dependent variable depends on independent variable

- The term independent can be confusing because it can be used strictly or loosely:
    - When doing experiment the variables that are actually manipulated are called "independent", here they really are truly independent, because they were set by experimenter
    - Also, non-manipulated values can be referred to as "independent", merely to indicate that they are being used as a predictor variables

- **Among non-manipulated variables, the roles of predicted and predictor variables are arbitrary**. It's just mathematical dependency not an underlying causal relationship

    
???

- consider human height and weight, you can use these variables in either way. 


---

## Summary of predicted and predictors

```{r}
knitr::include_graphics("https://imgs.xkcd.com/comics/correlation.png")
```

---

## Items can be measured on different scales 

- Whenever we are choosing a model for data, we must answer the question, what kind of scale are we dealing with? 

metric | ordinal | nominal
-------|---------|--------
response time, temperature, height, weight, counts | placing in a race, ratings | political party affiliation, face of a rolled dice/coin

- When two items have different nominal values, all we know is that the two items are different  
- If the two items have different ordinal values, we know that the two items are different and we know which one is "larger" than the other one, but not how much 
- When we have different metric values , then we know that they are different, which one is larger, and how much larger

???

- For example, participants in a run can be measured by the time they took to run the race, or by their placing in the race (first, second third, etc), or by the name of the team they represent. 
- These three measurements represent metric, ordinal, and nominal scales, respectively



