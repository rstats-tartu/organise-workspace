---
title: "Quick intro to statistics"
author: "Taavi Päll, Ülo Maiväli"
date: "2018-JUN-05 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, warn = -1)
knitr::opts_chunk$set(echo = FALSE, message = FALSE, fig.align = 'center', dev = 'svg', fig.height = 6)
library(tidyverse)
ggplot2::theme_set(theme_gray(base_size = 18))
```

class: inverse, center, middle

# What is statistics

---

## Questions that statistics asks

- How your data looks like and what are the parameter values describing your distribution? *For example, mean, median, variation, covariation*

- What should we believe, based on our sample, about the real parameter value in the whole population? *For example, when in our sample mean height is 178 cm, then what is the probability that real mean height of the population is > 185 cm?*

- Does the structure of our statistical model support our scientific hypothesis? *For example, when covariance of weight and height can be described by linear regression model, then we can prefer scientific hypothesis that provides mechanism for such relationship*

- What and how well our model predicts the future? *For example, can our model reasonably well predict outcome using new data?*

???

- For example, covariation of measured weight and height can be measured with help of correlation coeficient.

---

## Statistics answers scientific questions indirectly

- Main function of statistics is to quantitate and control the uncertainty when we seek answers to our questions

- Statistics answers scientific questions indirectly, like what is the probability of data given that null hypothesis is true

- Answers that statistics provides remain always confined to our data and models, which puts large emphasis on representative sampling. *Therefore one should always prefer well collected, rich data and flexible models in order to keep divergency between model coeficients and real world as small as possible (better data means better models)*

- Statistical inferences are always uncertain, there is always danger that we have done the wrong choice

---

## Branches of statistics

Statistics can be divided into three broad categories: descriptive, exploratory, and inferential:

1. Descriptive statistics includes summary statistics that are used to summarise a set of observations, in order to communicate the largest amount of information as simply as possible 

1. Exploratory stats creates new hypotheses based on your data by using iterative visualisation and modeling 

1. Inferential stats consists formal framework to test proposed hypotheses

---
class: inverse, center, middle

# Descriptive stats

---

## Summary stats characterise your sample

- Typical observation of the sample (mean, median, mode)

- Variance of observations (standard deviation, median absolute deviation),

- Covariance of variables (correlation coeficient)

```{r, fig.height=3}
set.seed(506)
mode <-  function(x, adjust = 1) {
  x <- na.omit(x)
  dx <- density(x, adjust = adjust)
  dx$x[which.max(dx$y)]
}
d <- data_frame(values = rlnorm(1000, sdlog = 0.6))
ggplot(data = d) +
  geom_density(mapping = aes(x = values, y = ..density..)) +
  geom_vline(xintercept = median(d$values), linetype = "dashed", size = 1) +
  geom_vline(xintercept = mean(d$values), size = 1) +
  geom_vline(xintercept = mode(d$values), linetype = "dotted", size = 1) +
  labs(caption = "MEAN (solid line),\nMEDIAN (dashed line),\nMODE (dotted line) of an log-normal distribution.")
```

Which of these summary statistics represents best your intuition about central tendency of this distribution?

???

- Kui valim on normaaljaotusega (histogramm on sümmeetriline), hinda tüüpilist liiget läbi aritmeetilise keskmise (mean).

Muidu kasuta mediaani (median). Kui valim on liiga väike, et jaotust hinnata (aga > 4), eelista mediaani. Mediaani saamiseks järjestatakse mõõdetud väärtused suuruse järgi ja võetakse selle rea keskmine liige. Mediaan on vähem tundlik ekstreemsete väärtuste (outlierite) suhtes kui mean.

Valimi kõige levinumat esindajat iseloomustab mood ehk jaotuse tipp. Seda on aga raskem täpselt määrata ja mitmetipulisel jaotusel on mitu moodi. Töötamisel posterioorsete jaotustega on mood sageli parim lahendus.

---

## Skim your dataset

Let's have a look at the summary stats of some variables of Starwars movie characters from `tidyverse::starwars` data:

```{r}
knitr::include_graphics("img/skim.png")
```
 
---

## Skim your dataset

Let's have a look at the summary stats of some variables of Starwars movie characters from `tidyverse::starwars` data:

```{r}
knitr::include_graphics("img/skim-rect.png")
```
 
---

## Variance of random variables

- Arithmetic mean is accompanied with standard deviation (SD). *SD has same dimension as your data and mean*

- Preferred format for displaying mean and SD in text is: "**mean (SD)**" (not "mean ( $\pm$ SD)") 

- It's important to remember that **1 SD covers 68% of distribution**, 
- 2 SD - 96% and 
- 3 SD - 99%. *Normal distribution has "thin" tails and for example only 1 out of 1 million data points is 5 SD away from mean*

---

## Example: human IQ

- Human IQ is normally distributed with mean = 100 and sd = 15

- When your IQ is 115 (mean IQ of students admitting university is 1 SD away from population mean), then the probability that random person you encounter on the street has higher IQ than you is 16% ((100% - 68%) / 2 = 16%)


```{r, fig.height = 4}
iq <- data_frame(IQ = rnorm(1000, 100, 15))
iqd <- density(iq$IQ)
smarter_than_you <- data_frame(x = iqd$x, y = iqd$y) %>% 
  filter(x >= 115)
ggplot(data = iq) +
  geom_density(mapping = aes(x = IQ)) +
  geom_vline(xintercept = 115, linetype = "dashed", size = 1) +
  geom_ribbon(data = smarter_than_you, mapping = aes(x = x, ymax = y), ymin = 0, alpha = 0.5) +
  labs(title = "Normal distribution\nwith mean=100 and sd=15")
```

- This can be also calculated with `pnorm(115, 100, 15, lower.tail = FALSE)`

---

## When your distribution is not that "normal"

- When your real life distribution has long or "fat" tails or has outliers then formula for calculating SD for normal distribution $s = \sqrt{\frac{\sum_{i=1}^N (x_i- \bar x)^2} {N - 1}}$ will overestimate SD and sample variance

- Simplest way to fix this situation when there are only positive values in your dataset is to **log transform** your data

```{r, fig.height=3}
set.seed(123)
d <- data_frame(`log-normal` = rlnorm(1000)) %>% 
  mutate(`log-transformed` = log(`log-normal`)) %>% 
  gather()
ggplot(data = d) +
  geom_histogram(mapping = aes(x = value), bins = 30) +
  facet_wrap(~key, scales = "free_x")
```



???

- normal model predicts negative values with relatively high frequency!!!

- igale jaotusele, mida me oskame integreerida, saab ka integraali abil õige katvusega standardhälbe arvutada. 

- Seega tasub meeles pidada, et tavapärane viis standardhälbe arvutamiseks sd() abil kehtib normaaljaotuse mudeli piirides ja ei kusagil mujal!

- Siiski, kui arvutada standardhälbe sd()-ga, võib olla kindel, jaotusest sõltumata hõlvavad 2 SD-d vähemalt 75% andmejaotusest. 


---

## SD calculated using original log-normal data is way off

```{r, fig.height=5}
original <- filter(d, key == "log-normal")
lower <- mean(original$value) - sd(original$value)
upper <- mean(original$value) + sd(original$value)
frac_data <- sum(original$value > lower & original$value < upper) / 1000
ggplot(data = original) +
  geom_density(mapping = aes(x = value)) +
  geom_vline(xintercept = c(lower,  upper), linetype = "longdash") +
  geom_label(label = frac_data, x = 1, y = 0.1) +
  labs(caption = "Dashed lines: 1*SD")
```

---

## Mean and SD from log transformed data

- Calculate mean and SD using log-transformed data and then convert them back to linear scale by taking anti-log. *For example, when $log_2(10) = 3.32$, then anti-log is $2^{3.32} = 10$*

- In this case, geometric mean and **geometric/multiplicative SD** is presented in the original scale:
    - multiplicative SD = geometric mean x SD; geometric mean / SD
    - however, geometric SD itself is multiplicative factor and therefore dimensionless
  - Geometric mean is always smaller than arthmetic mean 
  - SD interval is now assymetric and SD is always > 0

- This procedure ensures that ~68% of log normal data will remain in 1 SD range and ~96% data will remain into 2 SD range

.footnote[Example use of multiplicative SD in a research paper: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3876377/]

---

## Example calculation of multiplicative SD bounds

```{r, echo=TRUE}
# calculate mean and sd using log-trans data
# library(tidyverse)
set.seed(123)

d <- data_frame(`log-normal` = rlnorm(1000)) %>%
  mutate(`log-transformed` = log(`log-normal`)) %>%
  gather()

dsum <- filter(d, key == "log-transformed") %>% 
  summarise_at(vars(value), funs(mean, sd))

# convert calculated mean and sd back to original scale using exp
(bounds <- transmute_at(dsum, vars(mean, sd), exp) %>% 
  transmute(lower_1sd = mean / sd, upper_1sd = mean * sd,
            lower_2sd = mean / (2 * sd), upper_2sd = mean * (2 * sd)))
```

---

## Multiplicative SD placed over distribution

```{r, fig.height=5}
ggplot(data = original) +
  geom_density(mapping = aes(x = value)) +
  geom_vline(data = gather(select(bounds, ends_with("1sd"))), mapping = aes(xintercept = value), linetype = "longdash") +
  geom_vline(data = gather(select(bounds, ends_with("2sd"))), mapping = aes(xintercept = value), linetype = "dotted") +
  labs(caption = "Dashed lines: 1*SD\ndotted lines: 2*SD")
```

---

## Fraction of values within multiplicative SD bounds 

```{r, echo=TRUE}
# test calculated sd boundaries on the original data
filter(d, key == "log-normal") %>% 
  summarise(fraction_data_1SD = sum(value > bounds$lower_1sd & value < bounds$upper_1sd) / 1000,
            fraction_data_2SD = sum(value > bounds$lower_2sd & value < bounds$upper_2sd) / 1000)
```

.footnote[Code can be found at [rstats-tln/fmtdk-course/master/stats.Rmd](https://raw.githubusercontent.com/rstats-tln/fmtdk-course/master/stats.Rmd?token=AEl8JvIQ0g8KRAUmbaAytLfPJVFc8OTdks5bGTe0wA%3D%3D) file]

---

## Testing assumption of normality

- Certain tests, such as the t-test, require a normal distribution and therefore you might feel that you need to determine empirically whether your data has normal distribution

- Before you start with various normality tests, keep in mind that when your sample is not normal it does not mean automatically that the undelying population is also non-normal

- However, you would expect your sample to have the same distribution as the population. If it isn't, either your sample is very small or something went wrong somewhere..

- **Misconception: If your statistical analysis requires normality, it is a good idea to use a preliminary hypothesis test to screen for departures from normality**

.footnote[
[Do statistical tests require a normal distribution in the sample or in the population?](https://stats.stackexchange.com/q/289092/72998)
]

???

- Shapiro's test, Anderson Darling, and others are null hypothesis tests against the the assumption of normality. 

- These should not be used to determine whether to use normal theory statistical procedures. 

- In fact they are of virtually no value to the data analyst. 

- Under what conditions are we interested in rejecting the null hypothesis that the data are normally distributed? 

- I, personally, have never come across a situation where a normal test is the right thing to do. 

- **The problem is that when the sample size is small, even big departures from normality are not detected, and when your sample size is large, even the smallest deviation from normality will lead to a rejected null.**

https://www.r-bloggers.com/normality-tests-don%E2%80%99t-do-what-you-think-they-do/

---
### Q-Q plot to compare data to theoretical distribution

Q-Q plot allows visual evaluation of your variable against theoretical quantiles of a comparison distribution, e.g. normal distribution. R has at least two function to draw a Q-Q plot `qqplot` from **stats** package and `qqPlot` from **car** package. 

Let's compare the body mass index (BMI) of Starwars characters to normal distribution

```{r, fig.height=3.5}
library(car)
sw <- filter(starwars, !is.na(height), !is.na(mass)) %>% 
  mutate(BMI = mass / (height / 100)^2)
o <- qqPlot(sw$BMI, ylab = "Starwars character BMI")
```

.footnote[Seems like `r sw$name[o[1]]` is way off the line (15). You can make [Q-Q plot also in MS Excel](https://youtu.be/nX6-j6lY9qc). How to [interpret a Q-Q plot](https://stats.stackexchange.com/questions/101274/how-to-interpret-a-qq-plot)]

---

###  Normality tests are of virtually no value to the data analyst

- In case of Jabba the Hutt, even log-transformnation does not help to restore intergalactic BMI distribution close to normal. 
- However, we might expect that among Hutts in Nal Hutta (Jabbas' homeworld) BMI is still normally distributed with just very different parameters from many other species (e.g. mean BMI is 440-something)
- We can still go on with this data and model, for example, the relationship between BMI and species or eye-color

```{r, fig.height=3.5}
o <- qqPlot(log10(sw$BMI), ylab = "Starwars character BMI, Log10")
```


---

## Summary of descriptive statistics

- Descriptive statistics provides the first sanity check of your data

- Just by calculating simple arithmetic mean and standard deviation and looking at the distribution of your data (histogram and Q-Q plot) you can learn about your variables

- However, before you go on with exploring and modeling of data, it's important to understand **what types of variables** to you have in your dataset at hand and **which variables are dependent and which ones are independent**

---

class: inverse, center, middle

# Linear models

---

## Predictor and predicted variables

- Suppose we want to predict someone's weight from height. In this case, weight is the predicted variable and height is predictor variable. 

- The difference between predicted and predictor variables is that the predicted variables values are modeled as a function of values of predictor variables. 

- The value of the predictor variable comes from outside of the system being modeled, whereas the value of predicted variable depends on the value of predictor variable

- Therefore, mathematically, not in real life, predictor variable can be called **"independent"** variable and predicted variable can be called **"dependent"** variable


???

- Or, suppose we want to predict average score of an elementary school student based on PISA test result and family income, in this case average score is dependent variable and PISA test result and family income are independent variables. 

- Or, suppose we want to predict blood pressure of patients who either take drug A, or take drug B, or take a placebo, or merely wait. In this case blood pressure is the predicted variable and treatment category is the predictor

---

```{r}
ggplot(data = sw, mapping = aes(x = height, y = log(mass))) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "weight ~ height", 
       x = "Predictor (height)", 
       y = "Predicted (weight), Log", 
       caption = "Starwars data")
```


---

## Identify predicted and predictors first

- **The first thing you do in statistical inference, is to identify what variables are you interested in predicting on the basis of what predictors** 

- To rephrase, the key conceptual difference between independent and dependent variables is that the value of dependent variable depends on independent variable

- The term independent can be confusing because it can be used strictly or loosely:
    - When doing experiment the variables that are actually manipulated are called "independent", here they really are truly independent, because they were set by experimenter
    - Also, non-manipulated values can be referred to as "independent", merely to indicate that they are being used as a predictor variables

- **Among non-manipulated variables, the roles of predicted and predictor variables are arbitrary**. It's just mathematical dependency not an underlying causal relationship

    
???

- consider human height and weight, you can use these variables in either way. 


---

## Summary of predicted and predictors

```{r}
knitr::include_graphics("https://imgs.xkcd.com/comics/correlation.png")
```

---
class: inverse, center, middle

# Scales of variables

---

## Items can be measured on different scales 

- Whenever we are choosing a model for data, we must answer the question, what kind of scale are we dealing with? 


metric | ordinal | nominal
-------|---------|---------
response time, temperature, height, weight, counts | placing in a race, ratings | political party affiliation, face of a rolled dice/coin


---

## Levels of granularity

- When two items have different nominal values, all we know is that the two items are different  

- If the two items have different ordinal values, we know that the two items are different and we know which one is "larger" than the other one, but not how much 

- When we have different metric values , then we know that they are different, which one is larger, and how much larger

???

- For example, participants in a footrace can be measured by 

    1. the time they took to run the race, or 
    1. by their placing in the race (first, second third, etc), or 
    1. by the name of the team they represent

- These three measurements represent metric, ordinal, and nominal scales, respectively

---

## Choosing right test: nominal variables

(Non-exhaustive)
- Use the **exact binomial test** (small N) or **Chi-square test of goodness-of-fit** (large N) when you have one nominal variable with two or more values (such as red, pink and white flowers)

- Use the **Fisher's exact test of independence** when you have two nominal variables and you want to see whether the proportions of one variable are different depending on the value of the other variable. Use it when the sample size is small

.footnote[
[Testchoice](http://www.biostathandbook.com/testchoice.html)
]

???

- count the number of red, pink and white flowers in a genetic cross, test fit to expected 1:2:1 ratio, total sample <1000

---

### Observed and expected frequencies for leukemia

Treatment |Success|Failure|Row total
----------|-------|-------|---------
Prednisone| 14 (17.33) | 7 (3.67)| 21
Prednisone + VCR| 38 (34.67)| 4 (7.33) | 42
Column total| 52 | 11 | 63

- We want to test if the success and failure probabilities are the same for Prednisone and Prednisone + VCR

- H0 : Both treatment groups have equal success probability

.footnote[
Data: Tamhane and Dunlop 2000, p. 326
]

---

### Example: Fisher's Exact Test

```{r, echo=TRUE}
prednisone <- matrix(c(14, 7, 38, 4), nrow = 2, dimnames = list(c("Success", "Failure"), c("Prednisone", "Prednisone + VCR")))
fisher.test(prednisone)
```

---

### Example: Chi-squared test

```{r}
(xsq <- chisq.test(prednisone))
xsq$expected
```

---

## Choosing right test: Ranked variables

- One common application of the exact binomial test is known as the **sign test**. You use the sign test when there are two nominal variables and one measurement variable. Test randomness of direction of difference in paired data

- The data for a sign test could be also analysed using a paired t–test or a **Wilcoxon signed-rank test**, if the null hypothesis is that the mean or median difference between pairs of observations is zero. However, sometimes you're not interested in the size of the difference, just the direction

- Use the **Kruskal–Wallis test** when you have one nominal variable and one ranked variable. It tests whether the mean ranks are the same in all the groups. It's similar to one-way anova, but can be used in cases when you are not sure whether normality assumption holds

???

- We are going to use Wilcoxon sign-rank test in your data examples!

## Choosing right test: metric variables

- Most common uses of linear models in bioscience are: 
    - Metric response and one or more metric predictors: linear model, t test
    - Metric response nominal predicor: linear model, anova (e.g. average height across both sexes)

---

## Simple linear model

- Let's say we measured length and weight of N persons and we are interested how weight influences the height
- Simplest model is height = weight (y = x) and this model predict that when Juhans weight = 80 kg, then Juhan is 80 cm long
- Here y is predicted and x predictor variable 
- This model has no coefficients

```{r, fig.height=4}
x <- 1:100
ggplot(data_frame(x)) + 
  geom_line(aes(x = x, y = x, group = 1)) + 
  ylim(0, 100) +
  labs(title = "y ~ x")
```

???

- Sirge lõikab y telge alati 0-s (mudeli keeles: sirge intercept ehk lõikepunkt Y teljel = 0) ja 
- tema tõusunurk saab olla ainult 45 kraadi (mudeli keeles: mudeli slope ehk tõus = 1). - Selle mudeli jäikus tuleneb sellest, et temas ei ole parameetreid, mille väärtusi me saaksime vabalt muuta ehk tuunida.

---

## Adding constant to linear model

- What happens when we add a constant **a** = 20 to x y = a + x
- Constant **a** determines the value of y, when x = 0, and is model coefficent - **intercept**

```{r, fig.height=4}
ggplot(data_frame(x)) + 
  geom_line(aes(x, y = 20 + x, group = 1)) + 
  ylim(0, 100) +
  labs(title = "y ~ 20 + x")
```

---

## Changing slope of linear model

- What happens when we don't add but multiply x with constant: y = b * x
- Now changes the slope of the line, when x increases one unit then y increases 3 units

```{r, fig.height=4}
ggplot(data_frame(x)) + 
  geom_line(aes(x, y = 3 * x, group = 1)) + 
  ylim(0, 100) +
  labs(title = "y ~ 3 * x")
```

---

## Simple linear model with two coefficents

```{r, fig.height=4, message=FALSE}
ggplot(data_frame(x)) + 
  geom_line(aes(x, y = x, group = 1), size = 2) + 
  geom_line(aes(x, y = 20 + x, group = 1), size = 2, color = "blue") + 
  geom_line(aes(x, y = 3 * x, group = 1), size = 2, color = "red") + 
  geom_line(aes(x, y = 20 + 3 * x, group = 1), size = 2, color = "green") + 
  ylim(0, 100) +
  labs(caption = "Black, y ~ x.\n Blue, y ~ 20 + x.\nRed, y ~ 3 * x.\nGreen, y ~ 20 + 3 * x")
```

---

## General mathematical form of linear function

- The general mathematical form of linear function is:

$$y = \beta_0 + \beta_1x$$

- The value of parameter $\beta_0$ is called the intercept because it's where the line intercects the y-axis when x = 0
- The value of parameter $\beta_1$ is called the slope because it indicates how much y increases when x increases by 1
- If we have more than one predictor variable, the function has form:

$$y = \beta_0 + \beta_1x_1 + \dots + \beta_Kx_K$$

---

## Linear function with nominal predictors

- In case of nominal predictors, each value of nominal predictor generates particular deflection of y away from its baseline level







